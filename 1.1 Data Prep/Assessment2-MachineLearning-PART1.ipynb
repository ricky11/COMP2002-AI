{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1 - Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 - Data Preparation\n",
    "There are 100 rows and 11 dimensions in the dataset. The data is 100% dense. Feature labels were added.\n",
    "\n",
    "Task is to predict the number of containers a ship can carry. \n",
    "The target is known hence this is supervised learning.\n",
    "\n",
    "From the inputs we want to get a output of the weighted sum : ùë¶ = ùëì(ùë•ùë§)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import necessary modules and Read data and add feature names\n",
    "- Call the head method to get a general overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "try:\n",
    "    cols = [\"IMO_NO.\", \"Vessel_Name\", \"Year_Built\", \"Gross_Tonnage\", \"Deadweight_Tonnage\", \"Length\", \"Beam\", \"Capacity_(TEU)\", \"Forward_Bays\", \"Center_Bays\", \"Aft_Bays\"]\n",
    "    data = pd.read_csv('containers.csv',names=cols)\n",
    "    display(data.head())\n",
    "except Exception as e:\n",
    "    print(\"There was an error loading the file, make sure it is in root: \",e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Explore the data**\n",
    "\n",
    "Our first step is to summarize & explore the DataFrame by  computing aggregations. We can do this by using the info method in Pandas. We can see that all data is non-null as expected, and we have 8 numeric values, one target, and one String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding outliers and inconsistent data** \n",
    "\n",
    "For each of these features above, comparing the max and 75% values, we can start to see a huge difference in the Beam (Width) feature. This confirms that there may be an error with some of the tuples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicate Vessel Identification Number (IMO)**\n",
    "\n",
    "Now lets count the vessel identification number, which should be unique for the given dataset, however we can easily spot a duplicate IMO vessel. An online search shows that this vessel was renamed/purchased, we will leave in this duplicate vessel. IMO No. 9314947 in the dataset.\n",
    "\n",
    "Let us investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['IMO_NO.'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs have a quick look at the distribution of the Beam feature by plotting a histograms, this will show us the vessel with the incorrect width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for col in data.iloc[:,2:11]:\n",
    "data['Beam'].hist(figsize=(5, 3), bins=30, edgecolor=\"black\", )\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "plt.title('Beam (width distribution)')\n",
    "plt.xlabel('Beam (width)')\n",
    "plt.ylabel(\"Sum of Totals\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fix the outlier**\n",
    "\n",
    "Here we can see one ship has a length and width of 300x290 meters. I've never seen a square vessel before, probably doesn't go very fast!  The outlier is identified as MSC Albany with IMO 9619438. Correct beam should be 48meters. (https://www.vesselfinder.com/vessels/details/9619438) Since we have good reason to believe this is factually incorrect data it would be appropriate to correct the Beam to 48 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outlier as index 32, width cannot be the same as the length of the ship.\n",
    "print(data.loc[32])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_beam_MSC_Albany = 48\n",
    "edited_data = data.copy()\n",
    "edited_data.at[32,'Beam'] = corrected_beam_MSC_Albany\n",
    "edited_data.loc[32]\n",
    "edited_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data (training & testing)**\n",
    "\n",
    "- 75% Training data (Default)\n",
    "- 25% Testing data (Default)\n",
    "- A high training  set of 75% will prevent overfitting*\n",
    "  underfitting is too simple\n",
    "- Shuffle the data\n",
    "\n",
    "By default train_test_split method shuffles the Dataframe randomly prior to splitting, hence we do not need to shuffle beforehand. Shuffling the data ensures that there are no patterns or structure in the order of the data that could *bias the results* of the model. It also ensures that both the training and testing dataset contains a good generalization of the model and is representative of the overall distribution of the vessel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = edited_data.copy() # Copy prevents mutation of the original dataset incase we need to revert changes.\n",
    "y = edited_data['Capacity_(TEU)'].copy() # Prevents mutation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)\n",
    "X_Test_With_All_Columns = X_test.copy() # Keep a copy of the X_Test before dropping columns & Normalization below\n",
    "\n",
    "# Log the number of training and testing data, you can see 75% for training,a nd 25% for testing.\n",
    "print(X_train.shape, X_test.shape,y_train.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop features that are not required to train the model, such as the target variable and the IMO_NO & Vessel_Name in order to lower dimension. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['IMO_NO.','Vessel_Name','Capacity_(TEU)']\n",
    "X_train.drop(columns=cols_to_drop, axis=1, inplace=True) #Dropped cols\n",
    "X_test.drop(columns=cols_to_drop, axis=1, inplace=True) #Dropped Cols\n",
    "\n",
    "print(len(X_train)) # 75% Training data\n",
    "print(len(X_test)) # 25% Testing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling using MinMaxScaler (Normalization)**\n",
    "\n",
    "Now let us normalize the data to ensure that the values share a common scale, this will reduce complexity and optimize the data for machine learning. In this case we will use the StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_Train = scaler.fit_transform(X_train)\n",
    "scaled_X_Test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our data is now loaded, normalized, split and prepared for modelling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1.2 REGRESSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipelines**\n",
    "\n",
    "We will be training our model on 3 regression algorithm's mentioned below. In order to efficiently load the models, we can use *make_pipeline from sklearn.pipeline*.\n",
    "\n",
    "- Random Forest (Decision Trees)\n",
    "- Multi Layer Perceptron  (MLP)\n",
    "- Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required dependencies\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set up pipelines for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Store all the pipeline inside a dictionary.\n",
    "pipelines = {\n",
    "    'Random_Forest': make_pipeline(RandomForestRegressor(random_state=0)),\n",
    "    'Multi_Layer_Perceptron' : make_pipeline(MLPRegressor(random_state=0)), # Early stopping prevents generalization, stops when predictions get worse\n",
    "    'Support_Vector_Regression' : make_pipeline(SVR())\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 3 algorithms contain tunable hyperparameters, instead of manually tuning parameter for our dataset, we can utilize sklearn GridSearchCV. \n",
    "\n",
    "GridSearchCV provides a exhaustive search *(2-10 mins depending on CPU power)* on our predefined parameters for each algorithm. This returns the best possible combination of hyperparameters for each of our 3 algorithms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up a hyperparameter grid, the model will go through each of the parameters one by one in order to find the best convergence.\n",
    "\n",
    "hyper_param_grid = {\n",
    "    'Random_Forest': {\n",
    "        'randomforestregressor__n_estimators':[50,100,200]\n",
    "    },\n",
    "    'Multi_Layer_Perceptron' : {\n",
    "        'mlpregressor__hidden_layer_sizes':[50,100],\n",
    "        'mlpregressor__solver':['adam','lbfgs'],\n",
    "        'mlpregressor__max_iter':[100,500,2000]\n",
    "    },\n",
    "    'Support_Vector_Regression': {\n",
    "        'svr__kernel':['sigmoid','linear','poly'],\n",
    "\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start Training\n",
    "\n",
    "Import Grid Search CV and run the pipelines to fit the models on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import NotFittedError \n",
    "import warnings #Import python warning package\n",
    "from sklearn.exceptions import ConvergenceWarning # Disable Convergence Warnings\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "fit_model = {} #Dictionary that holds our models\n",
    "\n",
    "for algo,pipeline in pipelines.items():\n",
    "    try:\n",
    "        model = GridSearchCV(pipeline,hyper_param_grid[algo], verbose=4,cv=5) #Logging & Cross Validation, uses a scoring of r2 by default\n",
    "        print('Training started for',algo,'...')\n",
    "        model.fit(scaled_X_Train,y_train.values)\n",
    "        fit_model[algo] = model\n",
    "\n",
    "        print (algo, 'has been fitted! üëè')\n",
    "        print (\"========================================\")\n",
    "    except NotFittedError as e:\n",
    "        print (\"Error detected\")\n",
    "        print(repr(e))\n",
    "\n",
    "print(\"All Training has been completed!! üëèüëè\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now completed training! Lets output the best scores, best model and best params for each algorithm (based on training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the best scores for each Algorithm, for this we are using the default of R2.\n",
    "print(\"R2 SCORE FOR EACH REGRESSION (Closer to 1 is better)\")\n",
    "print(\"----------------------------\\n\")\n",
    "print(\"Random Forest : \", fit_model['Random_Forest'].best_score_)\n",
    "print(\"MLP : \", fit_model['Multi_Layer_Perceptron'].best_score_)\n",
    "print(\"SVR : \",fit_model['Support_Vector_Regression'].best_score_)\n",
    "\n",
    "# Output the best possible parameters for each Algorithm\n",
    "print(\"\\nBEST HYPER PARAMETERS FOR EACH MODEL\")\n",
    "print(\"----------------------------------\\n\")\n",
    "print(fit_model['Random_Forest'].best_params_)\n",
    "print(fit_model['Multi_Layer_Perceptron'].best_params_)\n",
    "print(fit_model['Support_Vector_Regression'].best_params_)\n",
    "\n",
    "print(\"\\nBEST HYPER PARAMETERS FOR EACH MODEL\")\n",
    "print(\"----------------------------------\\n\")\n",
    "\n",
    "# Print the best estimator, this is the best hyperparmeters\n",
    "print(\"Best estimator:\", fit_model['Random_Forest'].best_estimator_)\n",
    "print(\"Best estimator:\", fit_model['Multi_Layer_Perceptron'].best_estimator_)\n",
    "print(\"Best estimator:\", fit_model['Support_Vector_Regression'].best_estimator_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above metrics, can deduce the following. (Final output may be differ due to random splitting, shuffling)\n",
    "\n",
    "- Best Model in terms of R^2 score for : Multi_Layer_Perceptron\n",
    "- Best score achieved: 0.99~\n",
    "- Best hyperparameters: {'mlpregressor__hidden_layer_sizes': 50, 'mlpregressor__max_iter': 100, 'mlpregressor__solver': 'lbfgs'}\n",
    "\n",
    "*Note this is only based on TRAINING DATA* (Expect lower scores on testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_score = -1\n",
    "\n",
    "for algo, model in fit_model.items():\n",
    "    score = model.best_score_\n",
    "    if score > best_score:\n",
    "        best_model = algo\n",
    "        best_score = score\n",
    "\n",
    "print(\"Best model in terms of R^2 score:\", best_model)\n",
    "print(\"Best score achieved:\", best_score)\n",
    "print(\"Best hyperparameters:\", fit_model[best_model].best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Capacity Predictions using Test Data for all 3 models, ordered by Algorithm (Descending)\n",
    "\n",
    "We can now use the above models with the best hyperparametrs to predict our unseen *TESTING DATA*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dictionary to hold all predictions and add them top a TOP10 dataframe\n",
    "predictions = {}\n",
    "Top10 = X_Test_With_All_Columns.loc[:,cols_to_drop]\n",
    "for algo,pipeline in pipelines.items():\n",
    "   predictions[algo] = fit_model[algo].predict(scaled_X_Test) # We now use our TESTING data to make predictions\n",
    "   Top10[algo] = predictions[algo]\n",
    "\n",
    "RandomForestSet = Top10.drop([\"Multi_Layer_Perceptron\",\"Support_Vector_Regression\"],axis=1).sort_values('Random_Forest', ascending=False).reset_index(drop=True).head(10)\n",
    "MLPSet = Top10.drop([\"Random_Forest\",\"Support_Vector_Regression\"],axis=1).sort_values('Multi_Layer_Perceptron', ascending=False).reset_index(drop=True).head(10)\n",
    "SVR = Top10.drop([\"Random_Forest\",\"Multi_Layer_Perceptron\"],axis=1).sort_values('Support_Vector_Regression', ascending=False).reset_index(drop=True).head(10)\n",
    "display(RandomForestSet, MLPSet, SVR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Assessment of regression on Testing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the actual vs prediction values for all testing data for each regression by two methods.\n",
    "\n",
    "- Using a easy to interpret line plot showing our predicted vs actual test data\n",
    "- Use a Box plot summarize the distribution of models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error,mean_absolute_percentage_error\n",
    "for algo in predictions:\n",
    "    print(algo.upper() + \" METRICS\")\n",
    "    print(\"R2 Score\", + r2_score(y_test.values,predictions[algo])) # Best possible score is 1\n",
    "    print(\"Mean Abs Pct Error:\", + mean_absolute_percentage_error(y_test.values,predictions[algo])) # Mean absolute percentage error (MAPE) regression loss.\n",
    "    print(\"Mean Abs Error:\", + mean_absolute_error(y_test.values,predictions[algo])) #  Gives the mean of absolute difference between model prediction and target value. Lower the better\n",
    "    print(\"Mean Squared Error:\", + mean_squared_error(y_test.values,predictions[algo])) # Lower the better\n",
    "    plt.plot(predictions[algo], \"r:\", label=\"Predicted\")\n",
    "    plt.plot( y_test.values, \"b-\", label=\"Actual\")\n",
    "    plt.legend()\n",
    "    plt.title(algo + ' vs Truth Test Data')\n",
    "    plt.xlabel(\"Testing data of 25 ships\")\n",
    "    plt.ylabel(\"Capacity (Target)\")\n",
    "    plt.show()\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately notice that all the above metrics are lower/less accurate then the training data, this is expected as the testing data has not been seen by the trained model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way would be to use Cross Validation and summarize the all 3 models using the best hyper parameters from the previous step. Then use a box plot to show the R2_Score & Mean Squared Error(MSE) for each model.\n",
    "Using cross validation will prevent overfitting of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results from GridSearchCV we know the best parameter are stored in *.best_esimator* method.\n",
    "\n",
    "- Random Forest hyper parameters: {'randomforestregressor__n_estimators': 100}\n",
    "\n",
    "- MLP hyper parameters: {'mlpregressor__hidden_layer_sizes': 50, 'mlpregressor__max_iter': 100, 'mlpregressor__solver': 'lbfgs'}\n",
    "\n",
    "- SVR hyperparamers: {'svr__kernel': 'poly'}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot showing R2 scores (Closer to 1.0 indicates a more accurate prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running cross validation for each models best metrics on testing data\n",
    " \n",
    "from sklearn.model_selection import cross_validate\n",
    "fig, axs = plt.subplots(len(fit_model), 1, figsize=(8, 20))\n",
    "fig.suptitle('Box plots of training and testing R2 scores', fontsize=14)\n",
    "\n",
    "# scoring = 'neg_mean_squared_error'\n",
    "for i,algo in enumerate(fit_model):\n",
    "    regression_model = fit_model[algo].best_estimator_ #Using the .best_estimator reached from the GridSearchCV fitting.\n",
    "    cv_results = cross_validate(estimator=regression_model, X=scaled_X_Test, y=y_test, cv=5, scoring='r2' ,return_train_score=True)\n",
    "    training = cv_results[\"train_score\"]\n",
    "    testing = cv_results[\"test_score\"]\n",
    "  \n",
    "    axs[i].boxplot([training, testing], labels=['Training', 'Testing'])\n",
    "    axs[i].set_title(algo) \n",
    "    axs[i].set_ylabel(\"R2 score(0-1, 1 is better)\")\n",
    "   \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot showing MSE scores (Lower scores are better)\n",
    "We need to negate negative mean square error score as the scikit learn API assumes that all scoring metrics should be maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running cross validation for each models best metrics on testing data\n",
    " \n",
    "from sklearn.model_selection import cross_validate\n",
    "fig, axs = plt.subplots(len(fit_model), 1, figsize=(8, 20))\n",
    "fig.suptitle('Box plots of training and testing MSE (Lower is better)', fontsize=14)\n",
    "\n",
    "# scoring = 'neg_mean_squared_error'\n",
    "for i,algo in enumerate(fit_model):\n",
    "    regression_model = fit_model[algo].best_estimator_ #Using the .best_estimator reached from the GridSearchCV fitting.\n",
    "    cv_results = cross_validate(estimator=regression_model, X=scaled_X_Test, y=y_test, cv=5, scoring='neg_mean_squared_error' ,return_train_score=True)\n",
    "    training = - cv_results[\"train_score\"]\n",
    "    testing = - cv_results[\"test_score\"]\n",
    "    axs[i].boxplot([training, testing], labels=['Training', 'Testing'])\n",
    "    axs[i].set_title(algo) \n",
    "    axs[i].set_ylabel(\"Mean Square Error (lower is better)\")\n",
    "   \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "We can see that our testing data results were not as accurate as our training data, this is as expected.  \n",
    "\n",
    "Based on the mean MSE score, Support Vector Regression shows to have a closely matching mean to our Training data, this suggests that the model is neither overfitting or undefitting, i.e the model is able to generalize well to new unseen data. Random forest has a lower overall MSE indicating that the model performs better for the given testing data with a narrower whisker variance. \n",
    "\n",
    "Overall it is important to have multiple evaluation metrics to assess the model prediction. Multi Layer perceptron seemed to have the widest variance and despite the high R2 score, would probably not performance well on  larger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
