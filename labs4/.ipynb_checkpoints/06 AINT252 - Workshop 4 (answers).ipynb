{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "This workshop builds on this week's lecture, which introduced the idea of _evolutionary computation_. In this session you will start to construct the building blocks required to optimise a problem using a hillclimber. You will work with a benchmark optimisation problem - the _OneMax_ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneMax\n",
    "The OneMax problem is a binary problem (its decision variables $x_d$ are either 1 or 0), and the objective function is as follows:\n",
    "$$ f(x) = \\sum^D_{d=1} x_d $$\n",
    "The problem can be optimised with any value of $D$ - the more decision variables you have, the more complex the optimisation problem. The problem is a maximisation function, and the maximum value is found when all $D$ decision variables is set to 1 (remember, that this is not information that the optimiser has!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Implement a Python function called ``onemax`` that takes as an argument a sequence (e.g. a list or Numpy array) of 1s and 0s and returns the fitness result (i.e., the sum of the bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onemax(x):\n",
    "    \"\"\"\n",
    "    The solution is a Numpy array called x.\n",
    "    \"\"\"\n",
    "    return x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Write a second function that generates a random solution -- so a random sequence (list or Numpy array) of 1s and 0s. The ``numpy.random`` package will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_binary_solution(D):\n",
    "    return np.random.randint(0,2,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the two functions.\n",
    "D = 10\n",
    "for i in range(5):\n",
    "    x = random_binary_solution(D)\n",
    "    y = onemax(x)\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Implement a __bit flip__ mutation operator.\n",
    " 1. Pick a random decision variable.\n",
    " 1. If the decision variable is 0, change it to 1. Otherwise, if the decision variable is 1, change it to 0.\n",
    " 1. Return the mutated solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(x):\n",
    "    xp = x.copy()\n",
    "    idx = np.random.randint(len(x))\n",
    "    xp[idx] = 1-xp[idx]\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Write a function called ``evolve`` that performs a single generation of optimisation. Remember, given a starting solution and corresponding fitness values, a single generation should do the following:\n",
    " 1. __Mutate__ each of the current solution and evaluate the mutant according to the fitness function.\n",
    " 1. __Select__ the parent solution for the next generation (which solution -- parent or child -- has the highest fitness?).\n",
    " \n",
    "At the end of the ``evolve`` function you should return the new parent solution and its corresponding fitness value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(parent, parent_fitness):\n",
    "    # Mutation & evaluation.\n",
    "    child = mutate(parent)\n",
    "    child_fitness = onemax(child)\n",
    "    # Selection.\n",
    "    # Return the new parent solutions (and fitness values).\n",
    "    if child_fitness > parent_fitness:\n",
    "        return child, child_fitness\n",
    "    return parent, parent_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Using the ``evolve`` function, write a function that runs the optimisation process for a number of generations. The number of generations should be specified with the argument ``Ngens``. The number of decision variables should be specified with the argument ``D``. The final solution should be returned (along with its fitness). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise(Ngens, D):\n",
    "    parent = random_binary_solution(D)\n",
    "    parent_fitness = onemax(parent)    \n",
    "    # Optimisation\n",
    "    for gen in range(Ngens):\n",
    "        parent, parent_fitness = evolve(parent, parent_fitness)\n",
    "    return parent, parent_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimiser.\n",
    "solution, fitness = optimise(100,10)\n",
    "print(solution, fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
